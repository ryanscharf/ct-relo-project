---
title: "CT-Relo-Project"
author: "frank-corrigan"
date: "1/3/2022"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# install packages
# if(!require("osmdata")) install.packages("osmdata")
# if(!require("tidyverse")) install.packages("tidyverse")
# if(!require("sf")) install.packages("sf")
# if(!require("ggmap")) install.packages("ggmap")
# if(!require("rgdal")) install.packages("rgdal")
# if(!require("geosphere")) install.packages("geosphere")
# if(!require("ggplot2")) install.packages("ggplot2")

# load packages
library(tidyverse)
library(osmdata)
library(sf)
# library(ggmap)
# library(rgdal)
# library(geosphere)
library(tmap)
library(tigris)
#library(furrr)
library(tidycensus)
#library(parallel)
library(rvest)
library(httr)
library(polite)
library(janitor)
library(units)
```

## Background

Document to reproduce metrics and visualizations used in this article: http://www.frank-corrigan.com/2022/01/03/where-should-we-live/

First order is to load disperate data files (including shape files for maps)...

```{r load_data, message=FALSE, warning=FALSE}
acs_yr <- 2021
acs_survey <- 'acs1'

median_home_value_co <-
  get_acs(
    geography = 'county',
    variables = c('B25077_001'),
    year = acs_yr,
    survey = acs_survey
  )

#filter states to contiguous US
states <- tigris::states(cb = T ) %>% 
  filter(!STATEFP %in% c('02', '15', '60', '66', '69', '72', '78')) %>% 
  st_transform(4326)

counties <- tigris::counties(state = states$STATEFP, cb = T)%>% 
  st_transform(4326)


```

## Getting Trails Data

You can also embed plots, for example:

```{r get_trails_data, message=FALSE, warning=FALSE}

path_count_fn <- function(x) {
  df <- x %>%
    #pluck(1) %>%
    add_osm_feature(key = "highway",
                    value = c("footway", "cycleway")) %>%
    osmdata_sf()
  
  temp_node_count <- nrow(df$osm_points)
  return(temp_node_count)
}

possib_path <- possibly(.f = path_count_fn)

 # working loop to get the osm data
# nodes_df <- data.frame(GEOID = NULL, nodes = NULL)
# 
# for( i in 1:nrow(counties)){
#    cat(paste0(i,'\n'))
#   
#   go <- st_drop_geometry(counties[i, 'GEOID'])
#   x <- opq(bbox = st_bbox(counties[i, ]))
#   
#   df <- possib_path(x)
#     
#   res = data.frame(GEOID = go, nodes = df)
#   
#   nodes_df <- bind_rows(nodes_df, res)
#   
# }


# this map technique is definitely not working for me.
# I think the osm endpoint doesn't like getting a ton of 
# simultaneous requests from the same ip
# 
# location <- purrr::map(counties$geometry, ~ opq(bbox = st_bbox(.x)))
# 
#  ncores <- parallel::detectCores() - 2
#   plan(multisession, workers = ncores)
#   
#   osm_results <- furrr::future_map(location, 
#                                    .f = ~definitely_get_data(path_count_fn(.x)), .progress = T) %>%
#     bind_rows()
#   
#   osm_results_j <- counties %>% 
#     st_drop_geometry() %>% 
#     select(GEOID) %>%
#     bind_cols(osm_results)

  # nodes_df %>% write_csv('county_node_count.csv')
  # beepr::beep(2)
   
  nodes_df <- read_csv(paste0(here::here(), '/data/county_node_count.csv'))
  
  trail_df <- counties %>% left_join(nodes_df, by = 'GEOID')

 # print(head(trail_df))

# trail_df <- read.csv("osm_path_node_count.csv")

```

## Getting Distance to Airport Data

```{r airports, message=FALSE, warning=FALSE}

airports <- read_csv("https://raw.githubusercontent.com/lxndrblz/Airports/main/airports.csv", 
    col_types = cols(time_zone_id = col_skip(), 
        url = col_skip())) %>% 
  rename(geometry = location) %>% 
  st_as_sf(wkt = 'geometry', crs = 4326)
  

airports <- airports %>% 
  filter(country_id == 'US',
         # latitude != 0, 
         # longitude != 0,
         !str_detect(name, 'AFB|AAF'),
         !is.na(icao))

# there's way too many nonpassenger/primary airports in the first list
# we will grab the wikipedia list of airpots, which looks reasonable 
# and use that to filter the geocoded list
# 
# https://ivelasq.rbind.io/blog/politely-scraping/

wiki_url <- 'https://en.wikipedia.org/wiki/List_of_airports_in_the_United_States'
url_bow <- polite::bow(wiki_url)

wiki_html <- polite::scrape(url_bow) %>%
  html_nodes('table.wikitable') %>%
  html_table(fill = T)

wiki_tbl <- wiki_html[[1]] %>% 
  janitor::clean_names() %>% 
  filter(faa != '')


airports <- airports %>% 
  filter(icao %in% wiki_tbl$icao)


distances <- st_distance(counties, airports)

#find minimum distance to airport 
min_dist <- do.call(pmin, as.data.frame(distances))
max_dist <- do.call(pmax, as.data.frame(distances))


trail_df <- trail_df %>% 
  mutate(
    min_dist_airport = set_units(min_dist, 'miles'),
    max_dist_airport = set_units(max_dist, 'miles')
    )


```

```{r hazard}
# from https://hazards.fema.gov/nri/data-resources
# RESL community resilience
# SOVI social vulnerability 
# EAL expected annual loss
# https://www.ncei.noaa.gov/access/billions/mapping

hazard <- read_csv(paste0(here::here(), '/data/NRI_Table_Counties.csv')) %>% 
  filter(STCOFIPS %in% counties$GEOID)


```

## Scrape Disc Golf Courses

```{r hunt down disc golf courses, message=FALSE, warning=FALSE} 
# centroid_latlon <- function(x){
#   df <- x %>% st_centroid() %>% st_coordinates()
#   c(df[2], df[1])
# }

pdga_url <- "https://www.pdga.com/course-directory/advanced?title=&field_course_location_country=US&field_course_location_locality=&field_course_location_administrative_area=All&field_course_location_postal_code=&field_course_type_value=All&rating_value=All&field_course_holes_value=All&field_course_total_length_value=All&field_course_target_type_value=All&field_course_tee_type_value=All&field_location_type_value=All&field_course_camping_value=All&field_course_facilities_value=All&field_course_fees_value=All&field_course_handicap_value=All&field_course_private_value=All&field_course_signage_value=All&field_cart_friendly_value=All&page={page}"
pdga_base <- 'https://www.pdga.com'

pdga_pages <- 151
page <- seq(0, pdga_pages, 1) 

get_courses <- function(p_url, pdga_base = 'https://www.pdga.com'){
  
  url_bow <- polite::bow(p_url)
  url_scrap <- url_bow %>% 
    polite::scrape()
  
  nams <- url_scrap %>%
    html_nodes('.views-field-title') %>% 
    html_text2()
  
  nams <- nams[2:51]
  
  links <- url_scrap %>% html_nodes('a') %>% html_attr('href')
  links <- links[121:170]
  links_p <- paste0(pdga_base, links)

data.frame(course = nams, 
           link = links_p)
}

get_coords <- function(link, sleep = 3){
  cat(paste0('\n', link))
  
  #url_bow <- polite::bow(link)
  
  #c <- url_bow %>% polite::scrape() %>% html_nodes('a') %>% html_attr('href')
  c <- link %>% read_html() %>% html_nodes('a') %>% html_attr('href')
  c <- c[str_detect(c, 'maps') & !is.na(c)][2]
  c <- str_extract(c, '(?<=q=).*') %>% str_split(',')
  c_lat <- c[[1]][1] %>% as.numeric()
  c_lon <- c[[1]][2] %>% as.numeric()
  
  gc()
  Sys.sleep(sleep)
  
  return(
    data.frame(link = link,
             latitude = c_lat,
             longitude = c_lon)
  )

}

# #takes 25 minutes
# courses <- map_df(glue(pdga_url), get_courses)
# courses %>% filter(!is.na(course)) %>% write_csv('courses.csv')
courses <- read_csv(paste0(here::here(), '/data/courses.csv')) %>% filter(!is.na(course))


# course_geo <- data.frame()
# 
# for(i in 1:length(courses$link)){
# 
#   link <- courses$link[i]
#   cat(paste0('\n', link))
#   
#   #url_bow <- polite::bow(link)
#   
#   #c <- url_bow %>% polite::scrape() %>% html_nodes('a') %>% html_attr('href')
#   c <- link %>% read_html() %>% html_nodes('a') %>% html_attr('href')
#   c <- c[str_detect(c, 'maps') & !is.na(c)][2]
#   c <- str_extract(c, '(?<=q=).*') %>% str_split(',')
#   c_lat <- c[[1]][1] %>% as.numeric()
#   c_lon <- c[[1]][2] %>% as.numeric()
#   
#   gc()
#   
#   df <- data.frame(link = link,
#              latitude = c_lat,
#              longitude = c_lon) 
# 
#   course_geo <- course_geo %>% bind_rows(df)
#   saveRDS(course_geo, 'course_geo.rds')
# 
#       
#   Sys.sleep(1)
#   
# }

#will take 5 days
#course_geo <- map(courses$link, .f = possibly(get_coords), .progress = T)
#saveRDS(course_geo, 'course_geo.rds')
# course_geo <- course_geo %>% bind_rows()

course_geo <- readRDS(paste0(here::here(), '/data/course_geo.rds'))

# courses %>% left_join(course_geo) %>% write_csv('courses_geo.csv')

courses <- courses %>% left_join(course_geo) %>% filter(!is.na(course), !is.na(latitude))

```

```{r aggregating county information, message=FALSE, warning=FALSE}
county_stats <- courses %>% 
  st_as_sf(coords = c('longitude', 'latitude'), crs = 4326) %>%
  st_join(counties) %>% 
  group_by(GEOID) %>% 
  summarize(dg_n = n()) %>% 
  st_drop_geometry() %>% 
  right_join(counties) %>% 
  mutate(dg_n = replace_na(dg_n, 0) ,dg_nsqm = dg_n / ALAND)

county_stats <- county_stats %>%
  left_join(trail_df %>% st_drop_geometry()) %>% 
  rename(hiking_trail_nodes = nodes) %>% 
  left_join(hazard, by = c('GEOID' = 'STCOFIPS'))

# metrics of interest:
# dg_n, hiking_trail_odes, min_dist_airport, RISK_SCORE
# need to nearestneighbors  dg_n, hiking_trail_odes

 
```





## Distributions

```{r distributions}

# first, bring the data together... 
my_spdf@data <- left_join(my_spdf@data, school_data, by="NAME00") %>%
  left_join(trail_df, by=c("NAME00"="town"))

# these are the columns we want to peek at
plot_dist_data <- my_spdf@data %>% select(adj_home_price, age, population_growth,
                                          population_density, school_outcomes, path_node_count,
                                          dist_to_airport)

library(reshape2)
plot_dist_data_melt <- melt(plot_dist_data)

# finally, plot histograms
ggplot(plot_dist_data_melt, aes(x=value)) + geom_histogram() + facet_wrap(~variable, scales="free")

```

## Setting Up for Maps!

``` {r map-setup}

# create lists for colors and range names
col_list <- c("#7b3294", "#c2a5cf", "#f7f7f7", "#a6dba0", "#008837")
labels_list <- c("Lowest", "Low", "Medium", "High", "Highest")

# add id field (allows for join to fortified df when creating maps)
my_spdf@data$id <- 0:(dim(my_spdf@data)[1]-1) 

```

## House Prices

``` {r house-prices,  fig.width = 10, fig.asp = .52}

# create dataset for map visualization
World2 <- fortify(my_spdf)
World2_join = plyr::join(x = World2, y = my_spdf@data, by="id") # join by id
World2_join <- World2_join %>% filter(NAME00 != "County subdivisions not defined")
World2_join$cost_bucket <- cut(World2_join$adj_home_price, 6)

ggplot() + 
  geom_polygon(data = World2_join, aes(x = long, y = lat, group = group, fill = cost_bucket), # fill by OCCURENCE
               colour = "black", size = 0.5) +
  scale_color_manual(values = col_list, labels = labels_list) +
  scale_fill_manual(values = col_list, labels = labels_list) +
  geom_text(data=my_spdf@data %>% filter(NAME00 != "County subdivisions not defined"), aes(INTPTLON00, INTPTLAT00, label = NAME00), size=2.5) +
  labs(fill="House Price Ranges") +
  annotate("label", x = -72.25, y = 41, size = 7, label = expression(atop("CT Home Prices by Town", "(~2019 adjusted for mill rates)"))) +
  annotate("text", x = -72.70, y = 41.18, label = "Long Island Sound") +
  theme(panel.background = element_blank(),
      axis.title=element_blank(),
      axis.text=element_blank(),
      axis.ticks=element_blank())

```

## Trail Systems

``` {r trail-systems,  fig.width = 10, fig.asp = .52}

# create dataset for map visualization
World2 <- fortify(my_spdf)
World2_join = plyr::join(x = World2, y = my_spdf@data, by="id") # join by id
World2_join <- World2_join %>% filter(NAME00 != "County subdivisions not defined")
World2_join$cost_bucket <- cut(World2_join$path_node_count, 5)

ggplot() + 
  geom_polygon(data = World2_join, aes(x = long, y = lat, group = group, fill = cost_bucket), # fill by OCCURENCE
               colour = "black", size = 0.5) +
  scale_color_manual(values = col_list, labels = labels_list) +
  scale_fill_manual(values = col_list, labels = labels_list) +
  geom_text(data=my_spdf@data %>% filter(NAME00 != "County subdivisions not defined"), aes(INTPTLON00, INTPTLAT00, label = NAME00), size=2.5) +
  labs(fill="Trail Count Ranges") +
  annotate("label", x = -72.25, y = 41, size = 7, label = expression(atop("CT Trail Node Count by Town", "(~2021 from OSM data)"))) +
  annotate("text", x = -72.70, y = 41.18, label = "Long Island Sound") +
  theme(panel.background = element_blank(),
      axis.title=element_blank(),
      axis.text=element_blank(),
      axis.ticks=element_blank())

```

``` {r trail-systems-vis}

# get centerpoints for town of interest
guilford <- c(-72.70609, 41.33903)
farmington <- c(-72.84305, 41.73042)
the_spot <- farmington

# define "radius" from which to count trail system nodes
coords <- matrix(c(the_spot[1] - 0.16, the_spot[1] + 0.16, the_spot[2] - 0.16, the_spot[2] + 0.16), byrow = TRUE, nrow = 2, ncol = 2, dimnames = list(c('x','y'),c('min','max')))
location <- coords %>% opq()

# get data from osm
data <- location %>%
  add_osm_feature(key = "highway", 
                  value = c("footway", "cycleway")) %>%
  osmdata_sf()

# register_google(key = "Aanrq39hrf7rabgwegs-adsfawe") # note this is random key (i.e. not real - get a Google Maps API credential and Enable Static Maps from Google Console)
# has_google_key()

mad_map <- get_map(location = c(lon = the_spot[1], lat = the_spot[2]), zoom = 12)

# final map
ggmap(mad_map)+
  geom_sf(data = data$osm_points,
          inherit.aes = FALSE,
          colour = "#238443",
          fill = "#004529",
          alpha = .1,
          size = 4,
          shape = 21) +
  labs(x = "", y = "", title = "Farmington") 

```



## Proximity to Airport

``` {r distance-to-airport,  fig.width = 10, fig.asp = .52}

# create dataset for map visualization
World2 <- fortify(my_spdf)
World2_join = plyr::join(x = World2, y = my_spdf@data, by="id") # join by id
World2_join <- World2_join %>% filter(NAME00 != "County subdivisions not defined")
World2_join$cost_bucket <- cut(World2_join$dist_to_airport, 5)

ggplot() + 
  geom_polygon(data = World2_join, aes(x = long, y = lat, group = group, fill = cost_bucket), # fill by OCCURENCE
               colour = "black", size = 0.5) +
  scale_color_manual(values = col_list, labels = labels_list) +
  scale_fill_manual(values = col_list, labels = labels_list) +
  geom_text(data=my_spdf@data %>% filter(NAME00 != "County subdivisions not defined"), aes(INTPTLON00, INTPTLAT00, label = NAME00), size=2.5) +
  labs(fill="Miles to Airport Ranges") +
  annotate("label", x = -72.25, y = 41, size = 7, label = expression("CT Distance to Airport by Town", "(~2021 from CT Shape File Data)")) +
  annotate("text", x = -72.70, y = 41.18, label = "Long Island Sound") +
  theme(panel.background = element_blank(),
        axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())

```
## Putting it All Together

``` {r overall-score, fig.width = 10, fig.asp = .52}

my_spdf@data$score <- scale(my_spdf@data$school_outcomes) * 0.35 +
  scale(my_spdf@data$path_node_count) * 0.35 -
  scale(my_spdf@data$adj_home_price) * 0.2 -
  scale(my_spdf@data$dist_to_airport) * 0.1

# create dataset for map visualization
World2 <- fortify(my_spdf)
World2_join = plyr::join(x = World2, y = my_spdf@data, by="id") # join by id
World2_join <- World2_join %>% filter(NAME00 != "County subdivisions not defined")
World2_join$cost_bucket <- cut(World2_join$score, 5)

ggplot() + 
  geom_polygon(data = World2_join, aes(x = long, y = lat, group = group, fill = cost_bucket), # fill by OCCURENCE
               colour = "black", size = 0.5) +
  scale_color_manual(values = col_list, labels = labels_list) +
  scale_fill_manual(values = col_list, labels = labels_list) +
  geom_text(data=my_spdf@data %>% filter(NAME00 != "County subdivisions not defined"), aes(INTPTLON00, INTPTLAT00, label = NAME00), size=2.5) +
  labs(fill="Overall Score Ranges") +
  annotate("label", x = -72.25, y = 41, size = 7, label = expression("CT Relo Score by Town")) +
  annotate("text", x = -72.70, y = 41.18, label = "Long Island Sound") +
  theme(panel.background = element_blank(),
        axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())

```