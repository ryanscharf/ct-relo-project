---
title: "CT-Relo-Project"
author: "frank-corrigan"
date: "1/3/2022"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# install packages
# if(!require("osmdata")) install.packages("osmdata")
# if(!require("tidyverse")) install.packages("tidyverse")
# if(!require("sf")) install.packages("sf")
# if(!require("ggmap")) install.packages("ggmap")
# if(!require("rgdal")) install.packages("rgdal")
# if(!require("geosphere")) install.packages("geosphere")
# if(!require("ggplot2")) install.packages("ggplot2")

# load packages
library(tidyverse)
library(osmdata)
library(sf)
# library(ggmap)
# library(rgdal)
# library(geosphere)
library(tmap)
library(tigris)
#library(furrr)
library(tidycensus)
#library(parallel)
library(rvest)
library(httr)
library(polite)
library(janitor)
library(units)
library(stars)
library(sfdep)


```

## Background

Document to reproduce metrics and visualizations used in this article: http://www.frank-corrigan.com/2022/01/03/where-should-we-live/

First order is to load disperate data files (including shape files for maps)...



The [American Community Survey](https://www.census.gov/programs-surveys/acs/about.html) is a survey that goes on every year and collects a ton of demograpic data.  Results are compiled into 1 year, 5 year, and 10 year cohorts.  I'm going to use the ACS to get median home value by county.  I'm going to target the 1-year survey since housing prices have skyrocketed since the start of Covid, but pulling 1 year ACS estimates leaves a lot of empty counties. 
This map has a bunch of gaps and is very close to being a r/PeopleLiveInCities chart

```{r load_data, message=FALSE, warning=FALSE}
#filter states to contiguous US
states <- tigris::states(cb = T ) %>% 
  filter(!STATEFP %in% c('02', '15', '60', '66', '69', '72', '78')) %>% 
  st_transform(4326)

counties <- tigris::counties(state = states$STATEFP, cb = T)%>% 
  st_transform(4326)


acs_yr <- 2021
acs_survey <- 'acs1'

median_home_value_co <-
  get_acs(
    geography = 'county',
    variables = c('B25077_001'),
    year = acs_yr,
    survey = acs_survey
  )


median_home_value_co %>% 
  left_join(counties, by = 'GEOID') %>% 
  st_sf() %>% 
  ggplot() + geom_sf(aes(fill = estimate)) + scale_fill_viridis_c()

```
To fill in the missing information, we'll pull ACS 5-year results and get the relative change between survey durations for the counties that exist in the ACS1. From there, we fill in the missing county values with the ACS 5-year values and then adjust it by the geographically nearest ACS1 county's percentage change difference between the two surveys.

```{r missing home values, message=FALSE, warning=FALSE}


median_home_value_co_5yr <- 
    get_acs(
    geography = 'county',
    variables = c('B25077_001'),
    year = acs_yr,
    survey = 'acs5'
  )

mhv <- median_home_value_co %>%
  right_join(median_home_value_co_5yr, by = 'GEOID') %>% 
  mutate(inc = estimate.x/estimate.y,
         diff = estimate.x - estimate.y) %>%
  select(-NAME.y, 
         -variable.y, 
         NAME = NAME.x, 
         variable = variable.x,
  ) %>% 
  left_join(counties, by = 'GEOID') %>% 
  st_sf()
 
m_miss <- mhv %>% 
  filter(
    !is.na(NAME.x)
    )

nearest_acs1 <- st_join(median_home_value_co_5yr %>% 
      filter(!GEOID %in% m_miss$GEOID) %>% 
      left_join(counties, by = 'GEOID') %>% 
      st_sf(),
    m_miss, join = st_nearest_feature
    ) %>% 
  filter(!is.na(STATEFP.x)) %>%
  mutate(estimate = estimate * inc) %>%
  select(GEOID = GEOID.x, estimate, inc) %>% st_drop_geometry()

nearest_acs1 %>% 
  left_join(counties) %>% st_sf() %>%
    ggplot() +
    geom_sf(aes(fill = inc)) +
    scale_fill_viridis_c() +
  geom_sf(data = states, fill = NA, color = 'black') +
  ggtitle('Previously Missing Counties with Newly Interpolated ACS Values')


mhv <- mhv %>% 
  left_join(nearest_acs1, by = 'GEOID') %>% 
  mutate(estimate = case_when(
    is.na(estimate.x) ~ estimate.y,
    T ~ estimate.x),
    inc = case_when(
      is.na(inc.x)~ inc.y,
      T ~inc.x
    )) %>% 
  select(GEOID, estimate, inc)

mhv %>% st_sf() %>%
  ggplot() +
  geom_sf(aes(fill = estimate)) + 
  scale_fill_viridis_c() +
  ggtitle('Completed ACS Estimates Dataset')
```

You easily see the boundaries, in the Great Plains region, of where the groupings for the nearest ACS1 counties exist.  This method for replacing values isn't perfect.  ACS samplings will have a bias toward places with larger populations but the odds are that I wouldn't really be interested in extremely low population areas anyway.


## Getting Trails Data

You can also embed plots, for example:

```{r get_trails_data, message=FALSE, warning=FALSE}

path_count_fn <- function(x) {
  df <- x %>%
    #pluck(1) %>%
    add_osm_feature(key = "highway",
                    value = c("footway", "cycleway")) %>%
    osmdata_sf()
  
  temp_node_count <- nrow(df$osm_points)
  return(temp_node_count)
}

possib_path <- possibly(.f = path_count_fn)

 # working loop to get the osm data
# nodes_df <- data.frame(GEOID = NULL, nodes = NULL)
# 
# for( i in 1:nrow(counties)){
#    cat(paste0(i,'\n'))
#   
#   go <- st_drop_geometry(counties[i, 'GEOID'])
#   x <- opq(bbox = st_bbox(counties[i, ]))
#   
#   df <- possib_path(x)
#     
#   res = data.frame(GEOID = go, nodes = df)
#   
#   nodes_df <- bind_rows(nodes_df, res)
#   
# }


# this map technique is definitely not working for me.
# I think the osm endpoint doesn't like getting a ton of 
# simultaneous requests from the same ip
# 
# location <- purrr::map(counties$geometry, ~ opq(bbox = st_bbox(.x)))
# 
#  ncores <- parallel::detectCores() - 2
#   plan(multisession, workers = ncores)
#   
#   osm_results <- furrr::future_map(location, 
#                                    .f = ~definitely_get_data(path_count_fn(.x)), .progress = T) %>%
#     bind_rows()
#   
#   osm_results_j <- counties %>% 
#     st_drop_geometry() %>% 
#     select(GEOID) %>%
#     bind_cols(osm_results)

  # nodes_df %>% write_csv('county_node_count.csv')
  # beepr::beep(2)
   
  nodes_df <- read_csv(paste0(here::here(), '/data/county_node_count.csv'))
  
  
  
  
  trail_df <- counties %>% left_join(nodes_df, by = 'GEOID')
  
  trail_df <- trail_df %>%
  mutate(nodes = replace_na(nodes, 0)) %>% 
    filter(!is.na(GEOID)) %>%
    mutate(
        nb = st_contiguity(geometry) %>% include_self(), 
        neighbor_nodes = st_nb_apply(
          nodes, 
          nb, 
          NA, 
          .f  = function(.xij,...) sum(.xij))
        ) %>% 
    st_sf() %>%
  select(-nb) 

 # print(head(trail_df))

# trail_df <- read.csv("osm_path_node_count.csv")

```

## Getting Distance to Airport Data

```{r airports, message=FALSE, warning=FALSE}

airports <- read_csv("https://raw.githubusercontent.com/lxndrblz/Airports/main/airports.csv", 
    col_types = cols(time_zone_id = col_skip(), 
        url = col_skip())) %>% 
  rename(geometry = location) %>% 
  st_as_sf(wkt = 'geometry', crs = 4269)
  

airports <- airports %>% 
  filter(country_id == 'US',
         # latitude != 0, 
         # longitude != 0,
         !str_detect(name, 'AFB|AAF'),
         !is.na(icao))

# there's way too many nonpassenger/primary airports in the first list
# we will grab the wikipedia list of airpots, which looks reasonable 
# and use that to filter the geocoded list
# 
# https://ivelasq.rbind.io/blog/politely-scraping/

wiki_url <- 'https://en.wikipedia.org/wiki/List_of_airports_in_the_United_States'
url_bow <- polite::bow(wiki_url)

wiki_html <- polite::scrape(url_bow) %>%
  html_nodes('table.wikitable') %>%
  html_table(fill = T)

wiki_tbl <- wiki_html[[1]] %>% 
  janitor::clean_names() %>% 
  filter(faa != '')


airports <- airports %>% 
  filter(icao %in% wiki_tbl$icao)


distances <- st_distance(counties, airports)

#find minimum distance to airport 
min_dist <- do.call(pmin, as.data.frame(distances))
max_dist <- do.call(pmax, as.data.frame(distances))

#adding distances as new columns
trail_df <- trail_df %>% 
  mutate(
    min_dist_airport = set_units(min_dist, 'miles'),
    max_dist_airport = set_units(max_dist, 'miles')
    )

 tm_shape(counties) + tm_borders(col = 'grey') + 
   tm_shape(states) + tm_borders() + 
   tm_shape(airports) + tm_dots(col = 'red', size = .1)

```


The [UTCI](https://climate-adapt.eea.europa.eu/en/metadata/indicators/thermal-comfort-indices-universal-thermal-climate-index-1979-2019) (Universal Thermal Climate Index) is a standardized index to try to account for a universal human experience of heat which takes into account a large number of variables.  You can check out [Randy Au's post on this](https://counting.substack.com/p/what-goes-into-this-heat-index-thing) if you really a quick overview of it.  

Luckily, there's already a dataset avaliable from the European Union Space Programme's Copernicus Programme.  They use a bunch of satellites to generate tons datasets and make them avaliable for free.  In the case of the UTCI, they provide hourly UTCI rasters from [ERA-5](https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5). In this case, I didn't know what I was doing, so I grabbed the intermediate dataset.  This dataset contained a netCDF file for each day between September 30th 2019 and May 15th 2023.  Using a short time horizon leaves you very vulnerable to yearly weather variance from things like the El Nino Southern Oscillation. I'd like to use a larger sampling for time, but the archive for this time period was 55gb and took ~8 hrs to process.  I can live with these results and just keep in mind the asterisks around the data.

Each of those files have hourly observations, so I had to aggregate each day into a single metric of how many hours each grid cell had the 'optimal' index.  In the case of the UTCI, the optimal index is something like 50F - 78F.

This is a large range, to me, but I'm not an expert at this data, so I'm quite happy to just take this at face value.  I work with weather data a fair bit for my job, but I've learned that this domain is just niche enough that it can be really hard to find examples and documentation on how to do things. {stars} is a great package for working with spatial/temporal rasters, but there is not a large corpus of examples for people to learn off of. Check out the contents of utci.R if you want to see the details of how this aggregation was done.

```{r utci data aggregation, message = F, warning = F, eval = F}

categories <- data.frame(
  stringsAsFactors = FALSE,
          category = c("Extreme Heat Stress",
                       "Very Strong Heat Stress","Strong Heat Stress",
                       "Moderate Heat Stress","No Thermal Stress","Slight Cold Stress",
                       "Moderate Cold Stress","Strong Cold Stress",
                       "Very Strong Cold Stress","Extreme Cold Stress"),
               low = c(46L, 38L, 32L, 26L, 9L, 0L, -13L, -26L, -39L, -274L),
              high = c(999L, 45L, 37L, 31L, 25L, 8L, -1L, -14L, -27L, -40L)
)

good_index <- function(x){
      ifelse(
        between(x[[1]],0,9),1L,0L
        )
}

path <- paste0(here::here(),'/data/dataset-derived-utci-historical.zip')
files <- unzip(path, list = T)
files <- files %>% arrange(Name)

#drop first and last files. they're incomplete
files <- files[c(-1, -nrow(files)), ]

agg_gi <- function(fl, 
         path = paste0(here::here(),'/data/dataset-derived-utci-historical.zip'),
         output_file = paste0(here::here(),'utci_daily_good_index.rds')){
  
  cat(paste0('\n', fl, '\n'))
  
  dir <- tempdir()
  
  file <- unzip(path, file = fl, exdir = dir)
  
  nc <- read_ncdf(paste(dir, fl, sep = '/'), var = 'utci')

  #convert K to C
  nc_c <- (nc - 273.15)
  
  
  # crop to continental US
  nc_c <- st_crop(nc_c,
                  st_bbox(
                    c(
                      xmin = -124.848974,
                      xmax = -66.885444,
                      ymax = 49.384358,
                      ymin = 24.396308
                    ),
                    crs = st_crs(4326)
                    )
                  )
  
  gi <- nc_c %>% aggregate(by = '1 hour', FUN = good_index)
  gi2 <- gi %>% aggregate(by = '1 day', sum, na.rm = T)
  
  gi2 <- aperm(gi2, c(2,3,1))
  gi2 <- st_set_dimensions(gi2, 3, 
                           values = 
                             lubridate::ymd(st_get_dimension_values(gi2, 'time')))
  st_crs(gi2) <- st_crs(4326)
  
  if(file.exists(output_file)){
    of <- readRDS(output_file)
    c(of,gi2, along = 3) %>%
      saveRDS(output_file)
  } else {
    saveRDS(gi2, output_file)
  }
}

purrr::walk(files$Name, agg_gi, .progress = T)

```



```{r comfort index, message = F,  warning = F}
#source('utci.R')

utci <- readRDS(paste0(here::here(), '/data/utci_daily_good_index.rds'))
utci <- st_transform(utci, 4269)

utci_a <- utci %>%  
  aggregate(by = '8 years', FUN = mean, na.rm = T) %>% 
  aggregate(by = st_geometry(counties), mean, na.rm = T) %>%
  st_as_sf() %>%
  rename(average_prime_utci =`2019-01-01`)

utci_counties <- counties %>% st_join(utci_a, join = st_covers)

# 126 NAs need to be filled in 
# we'll just do a NN fill again


utci_miss <- utci_counties %>% 
  filter(
    !is.na(average_prime_utci)
    )

nearest_utci <- utci_counties %>%
  filter(!GEOID %in% utci_miss$GEOID) %>%
  st_join(utci_miss, join = st_nearest_feature) %>%
  filter(!is.na(STATEFP.x)) %>%
  select(GEOID = GEOID.x, average_prime_utci = average_prime_utci.y) %>%
  st_drop_geometry()

utci_counties <- utci_counties %>% 
  left_join(nearest_utci, by = 'GEOID') %>% 
  mutate(average_prime_utci = case_when(
    is.na(average_prime_utci.x) ~ average_prime_utci.y,
    T ~ average_prime_utci.x)) %>% 
  select(-average_prime_utci.x, -average_prime_utci.y)

utci_counties %>% select(average_prime_utci) %>%
  plot(breaks = 'quantile')

#add to trailsdf
trail_df <- trail_df %>% left_join(st_drop_geometry(utci_counties))
```


```{r hazard}
# from https://hazards.fema.gov/nri/data-resources
# RESL community resilience
# SOVI social vulnerability 
# EAL expected annual loss
# https://www.ncei.noaa.gov/access/billions/mapping
# https://www.fema.gov/sites/default/files/documents/fema_national-risk-index_technical-documentation.pdf

hazard <- read_csv(paste0(here::here(), '/data/NRI_Table_Counties.csv')) %>% 
  filter(STCOFIPS %in% counties$GEOID)


```

## Scrape Disc Golf Courses

```{r hunt down disc golf courses, message=FALSE, warning=FALSE} 
# centroid_latlon <- function(x){
#   df <- x %>% st_centroid() %>% st_coordinates()
#   c(df[2], df[1])
# }

pdga_url <- "https://www.pdga.com/course-directory/advanced?title=&field_course_location_country=US&field_course_location_locality=&field_course_location_administrative_area=All&field_course_location_postal_code=&field_course_type_value=All&rating_value=All&field_course_holes_value=All&field_course_total_length_value=All&field_course_target_type_value=All&field_course_tee_type_value=All&field_location_type_value=All&field_course_camping_value=All&field_course_facilities_value=All&field_course_fees_value=All&field_course_handicap_value=All&field_course_private_value=All&field_course_signage_value=All&field_cart_friendly_value=All&page={page}"
pdga_base <- 'https://www.pdga.com'

pdga_pages <- 151
page <- seq(0, pdga_pages, 1) 

get_courses <- function(p_url, pdga_base = 'https://www.pdga.com'){
  
  url_bow <- polite::bow(p_url)
  url_scrap <- url_bow %>% 
    polite::scrape()
  
  nams <- url_scrap %>%
    html_nodes('.views-field-title') %>% 
    html_text2()
  
  nams <- nams[2:51]
  
  links <- url_scrap %>% html_nodes('a') %>% html_attr('href')
  links <- links[121:170]
  links_p <- paste0(pdga_base, links)

data.frame(course = nams, 
           link = links_p)
}

get_coords <- function(link, sleep = 3){
  cat(paste0('\n', link))
  
  #url_bow <- polite::bow(link)
  
  #c <- url_bow %>% polite::scrape() %>% html_nodes('a') %>% html_attr('href')
  c <- link %>% read_html() %>% html_nodes('a') %>% html_attr('href')
  c <- c[str_detect(c, 'maps') & !is.na(c)][2]
  c <- str_extract(c, '(?<=q=).*') %>% str_split(',')
  c_lat <- c[[1]][1] %>% as.numeric()
  c_lon <- c[[1]][2] %>% as.numeric()
  
  gc()
  Sys.sleep(sleep)
  
  return(
    data.frame(link = link,
             latitude = c_lat,
             longitude = c_lon)
  )

}

# #takes 25 minutes
# courses <- map_df(glue(pdga_url), get_courses)
# courses %>% filter(!is.na(course)) %>% write_csv('courses.csv')
courses <- read_csv(paste0(here::here(), '/data/courses.csv')) %>% filter(!is.na(course))


# course_geo <- data.frame()
# 
# for(i in 1:length(courses$link)){
# 
#   link <- courses$link[i]
#   cat(paste0('\n', link))
#   
#   #url_bow <- polite::bow(link)
#   
#   #c <- url_bow %>% polite::scrape() %>% html_nodes('a') %>% html_attr('href')
#   c <- link %>% read_html() %>% html_nodes('a') %>% html_attr('href')
#   c <- c[str_detect(c, 'maps') & !is.na(c)][2]
#   c <- str_extract(c, '(?<=q=).*') %>% str_split(',')
#   c_lat <- c[[1]][1] %>% as.numeric()
#   c_lon <- c[[1]][2] %>% as.numeric()
#   
#   gc()
#   
#   df <- data.frame(link = link,
#              latitude = c_lat,
#              longitude = c_lon) 
# 
#   course_geo <- course_geo %>% bind_rows(df)
#   saveRDS(course_geo, 'course_geo.rds')
# 
#       
#   Sys.sleep(1)
#   
# }

#will take 5 days
#course_geo <- map(courses$link, .f = possibly(get_coords), .progress = T)
#saveRDS(course_geo, 'course_geo.rds')
# course_geo <- course_geo %>% bind_rows()

course_geo <- readRDS(paste0(here::here(), '/data/course_geo.rds'))

# courses %>% left_join(course_geo) %>% write_csv('courses_geo.csv')

courses_sf <- courses %>% 
  left_join(course_geo) %>% 
  filter(!is.na(course), !is.na(latitude)) %>%
  st_as_sf(coords = c('longitude', 'latitude'), crs = 4269) 

dg_counties <- courses_sf %>%
  st_join(counties) %>% 
  group_by(GEOID) %>%
  summarize(n_dg = n()) %>% 
  st_drop_geometry() %>%
  full_join(counties)


dg_counties <- dg_counties %>%
  mutate(n_dg = replace_na(n_dg, 0)) %>% 
    filter(!is.na(GEOID)) %>%
    mutate(
        nb = st_contiguity(geometry) %>% include_self(), 
        neighbor_dg = st_nb_apply(
          n_dg, 
          nb, 
          NA, 
          .f  = function(.xij,...) sum(.xij, na.rm = T))
        ) %>% 
  select(GEOID, neighbor_dg, n_dg) 

#add nn

trail_df <- trail_df %>% left_join(dg_counties, by = 'GEOID')
```


```{r aggregating county information, message=FALSE, warning=FALSE}


county_stats <- trail_df %>%
  rename(hiking_trail_nodes = nodes) %>% 
  left_join(hazard, by = c('GEOID' = 'STCOFIPS')) %>%
  left_join(st_drop_geometry(mhv))

# metrics of interest:
# neighbor_nodes, min_dist_airport, average_prime_utci, neighbor_dg,
# RISK_SCORE, estimate

cs <- county_stats %>% 
  mutate(
    across(c(
    neighbor_nodes, 
    min_dist_airport, 
    average_prime_utci, 
    neighbor_dg, 
    RISK_SCORE, 
    estimate),
    percent_rank,
    .names = '{.col}_percentile')
    ) %>% 
  select(
    NAME,
    NAMELSAD,
    GEOID,
    STUSPS,
    STATE_NAME,
    ALAND,
    neighbor_nodes, 
    min_dist_airport, 
    average_prime_utci, 
    neighbor_dg, 
    RISK_SCORE, 
    RISK_RATNG,
    estimate,
    neighbor_nodes_percentile, 
    min_dist_airport_percentile, 
    average_prime_utci_percentile, 
    neighbor_dg_percentile, 
    RISK_SCORE_percentile, 
    estimate_percentile) %>% 
  mutate(RISK_SCORE_percentile = abs(1-RISK_SCORE_percentile),
         min_dist_airport_percentile = abs(1-min_dist_airport_percentile),
         estimate_percentile = abs(1-estimate_percentile))


 saveRDS(cs, paste0(here::here(), '/data/county_stats.rds'))
```





## Distributions

```{r distributions}

# first, bring the data together... 
my_spdf@data <- left_join(my_spdf@data, school_data, by="NAME00") %>%
  left_join(trail_df, by=c("NAME00"="town"))

# these are the columns we want to peek at
plot_dist_data <- my_spdf@data %>% select(adj_home_price, age, population_growth,
                                          population_density, school_outcomes, path_node_count,
                                          dist_to_airport)

library(reshape2)
plot_dist_data_melt <- melt(plot_dist_data)

# finally, plot histograms
ggplot(plot_dist_data_melt, aes(x=value)) + geom_histogram() + facet_wrap(~variable, scales="free")

```

## Setting Up for Maps!

``` {r map-setup}

# create lists for colors and range names
col_list <- c("#7b3294", "#c2a5cf", "#f7f7f7", "#a6dba0", "#008837")
labels_list <- c("Lowest", "Low", "Medium", "High", "Highest")

# add id field (allows for join to fortified df when creating maps)
my_spdf@data$id <- 0:(dim(my_spdf@data)[1]-1) 

```

## House Prices

``` {r house-prices,  fig.width = 10, fig.asp = .52}

# create dataset for map visualization
World2 <- fortify(my_spdf)
World2_join = plyr::join(x = World2, y = my_spdf@data, by="id") # join by id
World2_join <- World2_join %>% filter(NAME00 != "County subdivisions not defined")
World2_join$cost_bucket <- cut(World2_join$adj_home_price, 6)

ggplot() + 
  geom_polygon(data = World2_join, aes(x = long, y = lat, group = group, fill = cost_bucket), # fill by OCCURENCE
               colour = "black", size = 0.5) +
  scale_color_manual(values = col_list, labels = labels_list) +
  scale_fill_manual(values = col_list, labels = labels_list) +
  geom_text(data=my_spdf@data %>% filter(NAME00 != "County subdivisions not defined"), aes(INTPTLON00, INTPTLAT00, label = NAME00), size=2.5) +
  labs(fill="House Price Ranges") +
  annotate("label", x = -72.25, y = 41, size = 7, label = expression(atop("CT Home Prices by Town", "(~2019 adjusted for mill rates)"))) +
  annotate("text", x = -72.70, y = 41.18, label = "Long Island Sound") +
  theme(panel.background = element_blank(),
      axis.title=element_blank(),
      axis.text=element_blank(),
      axis.ticks=element_blank())

```

## Trail Systems

``` {r trail-systems,  fig.width = 10, fig.asp = .52}

# create dataset for map visualization
World2 <- fortify(my_spdf)
World2_join = plyr::join(x = World2, y = my_spdf@data, by="id") # join by id
World2_join <- World2_join %>% filter(NAME00 != "County subdivisions not defined")
World2_join$cost_bucket <- cut(World2_join$path_node_count, 5)

ggplot() + 
  geom_polygon(data = World2_join, aes(x = long, y = lat, group = group, fill = cost_bucket), # fill by OCCURENCE
               colour = "black", size = 0.5) +
  scale_color_manual(values = col_list, labels = labels_list) +
  scale_fill_manual(values = col_list, labels = labels_list) +
  geom_text(data=my_spdf@data %>% filter(NAME00 != "County subdivisions not defined"), aes(INTPTLON00, INTPTLAT00, label = NAME00), size=2.5) +
  labs(fill="Trail Count Ranges") +
  annotate("label", x = -72.25, y = 41, size = 7, label = expression(atop("CT Trail Node Count by Town", "(~2021 from OSM data)"))) +
  annotate("text", x = -72.70, y = 41.18, label = "Long Island Sound") +
  theme(panel.background = element_blank(),
      axis.title=element_blank(),
      axis.text=element_blank(),
      axis.ticks=element_blank())

```

``` {r trail-systems-vis}

# get centerpoints for town of interest
guilford <- c(-72.70609, 41.33903)
farmington <- c(-72.84305, 41.73042)
the_spot <- farmington

# define "radius" from which to count trail system nodes
coords <- matrix(c(the_spot[1] - 0.16, the_spot[1] + 0.16, the_spot[2] - 0.16, the_spot[2] + 0.16), byrow = TRUE, nrow = 2, ncol = 2, dimnames = list(c('x','y'),c('min','max')))
location <- coords %>% opq()

# get data from osm
data <- location %>%
  add_osm_feature(key = "highway", 
                  value = c("footway", "cycleway")) %>%
  osmdata_sf()

# register_google(key = "Aanrq39hrf7rabgwegs-adsfawe") # note this is random key (i.e. not real - get a Google Maps API credential and Enable Static Maps from Google Console)
# has_google_key()

mad_map <- get_map(location = c(lon = the_spot[1], lat = the_spot[2]), zoom = 12)

# final map
ggmap(mad_map)+
  geom_sf(data = data$osm_points,
          inherit.aes = FALSE,
          colour = "#238443",
          fill = "#004529",
          alpha = .1,
          size = 4,
          shape = 21) +
  labs(x = "", y = "", title = "Farmington") 

```



## Proximity to Airport

``` {r distance-to-airport,  fig.width = 10, fig.asp = .52}

# create dataset for map visualization
World2 <- fortify(my_spdf)
World2_join = plyr::join(x = World2, y = my_spdf@data, by="id") # join by id
World2_join <- World2_join %>% filter(NAME00 != "County subdivisions not defined")
World2_join$cost_bucket <- cut(World2_join$dist_to_airport, 5)

ggplot() + 
  geom_polygon(data = World2_join, aes(x = long, y = lat, group = group, fill = cost_bucket), # fill by OCCURENCE
               colour = "black", size = 0.5) +
  scale_color_manual(values = col_list, labels = labels_list) +
  scale_fill_manual(values = col_list, labels = labels_list) +
  geom_text(data=my_spdf@data %>% filter(NAME00 != "County subdivisions not defined"), aes(INTPTLON00, INTPTLAT00, label = NAME00), size=2.5) +
  labs(fill="Miles to Airport Ranges") +
  annotate("label", x = -72.25, y = 41, size = 7, label = expression("CT Distance to Airport by Town", "(~2021 from CT Shape File Data)")) +
  annotate("text", x = -72.70, y = 41.18, label = "Long Island Sound") +
  theme(panel.background = element_blank(),
        axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())

```
## Putting it All Together

``` {r overall-score, fig.width = 10, fig.asp = .52}

my_spdf@data$score <- scale(my_spdf@data$school_outcomes) * 0.35 +
  scale(my_spdf@data$path_node_count) * 0.35 -
  scale(my_spdf@data$adj_home_price) * 0.2 -
  scale(my_spdf@data$dist_to_airport) * 0.1

# create dataset for map visualization
World2 <- fortify(my_spdf)
World2_join = plyr::join(x = World2, y = my_spdf@data, by="id") # join by id
World2_join <- World2_join %>% filter(NAME00 != "County subdivisions not defined")
World2_join$cost_bucket <- cut(World2_join$score, 5)

ggplot() + 
  geom_polygon(data = World2_join, aes(x = long, y = lat, group = group, fill = cost_bucket), # fill by OCCURENCE
               colour = "black", size = 0.5) +
  scale_color_manual(values = col_list, labels = labels_list) +
  scale_fill_manual(values = col_list, labels = labels_list) +
  geom_text(data=my_spdf@data %>% filter(NAME00 != "County subdivisions not defined"), aes(INTPTLON00, INTPTLAT00, label = NAME00), size=2.5) +
  labs(fill="Overall Score Ranges") +
  annotate("label", x = -72.25, y = 41, size = 7, label = expression("CT Relo Score by Town")) +
  annotate("text", x = -72.70, y = 41.18, label = "Long Island Sound") +
  theme(panel.background = element_blank(),
        axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())

```
